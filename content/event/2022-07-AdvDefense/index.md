---
title: ConvEBMDefense for Medical Images Analysis
event: PhD Candidacy

location: UCF, Orlando Campus, FL

summary: Deep adversarial defense for deep convolutional nerual networks.
abstract: Deep neural network (DNN) is the core of Artificial Intelligence (AI). It has achieved impressive performance on vision research, natural language processing and audio processing in tasks such as classification, segmentation, image in-painting, reconstruction, text-to-image conversion, etc. However, serious concerns were brought up due to the vulnerability of deep neural network. It is observed that attacked models have high confidence of wrong predictions of perturbed information. and the perturbations could continue to fool multiple models ([Szegedy et al., 2013], [Biggio et al., 2013], [Goodfellow et al., 2014b]). Small perturbations that human eyes couldn’t perceive could turn the state of the art (SOTA) accuracy of a deep neural network from 100% to 0%. Since this intriguing phenomenon was discovered, the number of published papers has grown exponentially. Medicine is a high stake field where the adversarial attacks can cause damages of millions of lives and encounter huge financial loss for institutions or organizations ([Finlayson et al., 2018]). Even though the vulnerability is also discovered recently in the medical domain and medical images has found to be extremely venerable than natural images ([Ma et al., 2021], [Hirano et al., 2020], [Hirano et al., 2021], [Bortsova et al., 2021],etc), there is no proper evaluated and sufficient defense that’s being available ([Apostolidis and Papakostas, 2021], [Hirano et al., 2020]). Inspired by Hill et al. [2020], we proposed Convergent Energy Based Model adversarial Defense(ConvEBMDefense) for medical imaging diagnostic system defense. We believe this is the first effective defense that can defend on medical imaging diagnostic system. Our naturally trained ConvEBMDefense has an average robust accuracy of 86.8\% with the known strongest attack on chest-xray pneumonia diagnosis ([Kermany et al, 2018a]).
  
date: '2022-07-28T12:30:00Z'
date_end: '2022-07-28T14:30:00Z'
all_day: false

# Schedule page publish date (NOT talk date).
publishDate: '2022-07-28T00:00:00Z'
authors: []
tags: [Deep Learning, EBM, Adversarial Attack, Adversarial Defense]


# Is this a featured talk? (true/false)
featured: false
image:
  caption: "Image credit: [**Gong et al., 2018**](https://www.researchgate.net/profile/Yuan-Gong-6/publication/325370539/figure/fig1/AS:641308850937856@1529911353222/An-illustration-of-machine-learning-adversarial-examples-Studies-have-shown-that-by.png)"
  focal_point: Center

#math: yes

links:
  - icon: flask
    icon_pack: fas
    name: slides
    url: ../slides/2022-07-AdvDefense/Proposal_presentation.pdf
    
---

{{% callout note %}}
Click on the **Slides** button above to view the built-in slides feature.
{{% /callout %}}


